seed: 42

Trainer:
  epochs: 600
  gpus: 1
  precision: 32
  checkpoint_dir: "tmp/checkpoint"
  accumulate_grad_batches: 1
  auto_lr_find: False
  auto_scale_batch_size: False
  fast_dev_run: False
  logger:
    wandb:
      use: True
      project: "TANet"
    log_every_n_steps: 5

LitDataModule:
  dataloader:
    batch_size: 8
    num_workers: 4
  dataset:
    teeth_num: 28
    sample_num: 512
    csv_dir: "dataset_csv"
    train:
      split: "train"
    val:
      split: "val"
      split_ratio: 0.25
    test:
      split: "test"
      split_ratio: 0.25

LitModule:
  optimizer:
    NAME: "adam"
    learning_rate: 0.0001
    weight_decay: 0.
    momentum: 0.9
  scheduler:
    NAME: "cosine"
    min_lr: 0
    warmup_start_lr: 0
    warmup_epochs: 0
    eta_min: 0

criterion:
  mode: "l2"
